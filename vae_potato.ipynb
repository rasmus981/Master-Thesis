{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "import pickle\n",
    "import warnings\n",
    "from scipy import misc\n",
    "import scipy.ndimage\n",
    "from skimage import feature\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, Lambda, Input, Conv2DTranspose, Reshape, UpSampling2D\n",
    "from keras.models import model_from_json, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_hollow_data(procent, model_number):\n",
    "    train_hollow_folder = \"potato_dataset/data_\" + str(procent) + \"/model_\" + str(model_number) + \"/train/hollow/\"\n",
    "    \n",
    "    train_data_hollow = []\n",
    "    \n",
    "    for filename in os.listdir(train_hollow_folder):\n",
    "        if filename != \".DS_Store\" and filename != \".png\" and filename != \".ipynb_checkpoints\":\n",
    "            img = (cv2.imread(train_hollow_folder + filename, cv2.IMREAD_GRAYSCALE) / 255.0)\n",
    "            train_data_hollow.append(img)\n",
    "            \n",
    "            \n",
    "    np_train_data_hollow = np.array(train_data_hollow)\n",
    "    \n",
    "    train_data = np_train_data_hollow.reshape(np_train_data_hollow.shape[0], 128, 128, 1)\n",
    "    \n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_metal_data(procent, model_number):\n",
    "    train_metal_folder = \"potato_dataset/data_\" + str(procent) + \"/model_\" + str(model_number) + \"/train/metal/\"\n",
    "    \n",
    "    train_data_metal = []\n",
    "    \n",
    "    for filename in os.listdir(train_metal_folder):\n",
    "        if filename != \".DS_Store\" and filename != \".png\" and filename != \".ipynb_checkpoints\":\n",
    "            img = (cv2.imread(train_metal_folder + filename, cv2.IMREAD_GRAYSCALE) / 255.0)\n",
    "            train_data_metal.append(img)\n",
    "            \n",
    "            \n",
    "    np_train_data_metal = np.array(train_data_metal)\n",
    "    \n",
    "    train_data = np_train_data_metal.reshape(np_train_data_metal.shape[0], 128, 128, 1)\n",
    "    \n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_perfect_data(procent, model_number):\n",
    "    train_perfect_folder = \"potato_dataset/data_\" + str(procent) + \"/model_\" + str(model_number) + \"/train/perfect/\"\n",
    "    \n",
    "    train_data_perfect = []\n",
    "    \n",
    "    for filename in os.listdir(train_perfect_folder):\n",
    "        if filename != \".DS_Store\" and filename != \".png\" and filename != \".ipynb_checkpoints\":\n",
    "            img = (cv2.imread(train_perfect_folder + filename, cv2.IMREAD_GRAYSCALE) / 255.0)\n",
    "            train_data_perfect.append(img)\n",
    "            \n",
    "            \n",
    "    np_train_data_perfect = np.array(train_data_perfect)\n",
    "    \n",
    "    train_data = np_train_data_perfect.reshape(np_train_data_perfect.shape[0], 128, 128, 1)\n",
    "    \n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a custom layer to calculate the loss\n",
    "class CustomVariationalLayer(keras.layers.Layer):\n",
    "\n",
    "    def vae_loss(self, x, z_decoded, z_log_sigma, z_mu):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        # Reconstruction loss\n",
    "        xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        # KL divergence\n",
    "        kl_loss = -5e-4 * K.mean(1 + z_log_sigma - K.square(z_mu) - K.exp(z_log_sigma), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    # adds the custom loss to the class\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        z_log_sigma= inputs[2]\n",
    "        z_mu = inputs[3]\n",
    "        loss = self.vae_loss(x, z_decoded, z_log_sigma, z_mu)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(train_data, procent, model, potato_type, number_of_images):\n",
    "    \n",
    "    # Standard INFO\n",
    "    img_shape = (128, 128, 1)\n",
    "    batch_size = 16\n",
    "    latent_dim = 100\n",
    "\n",
    "    # Encoder\n",
    "    input_img = Input(shape=img_shape)\n",
    "\n",
    "    x = Conv2D(32, kernel_size=3,\n",
    "                    padding='same', \n",
    "                    activation='relu',\n",
    "                    strides=(2,2))(input_img)\n",
    "    x = Conv2D(32, kernel_size=3,\n",
    "                    padding='same', \n",
    "                    activation='relu',\n",
    "                    strides=(2, 2))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "\n",
    "    # Two outputs, latent mean and (log)variance\n",
    "    z_mu = Dense(latent_dim)(x)\n",
    "    z_log_sigma = Dense(latent_dim)(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def sampling(args):\n",
    "        z_mu, z_log_sigma = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mu)[0], latent_dim),\n",
    "                                  mean=0., stddev=1.)\n",
    "        return z_mu + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "    # sample vector from the latent distribution\n",
    "    z = Lambda(sampling)([z_mu, z_log_sigma])\n",
    "    \n",
    "    \n",
    "    # Decoder\n",
    "    decoder_input = Input(K.int_shape(z)[1:])\n",
    "\n",
    "    # Expand to 32 * 32 * 8 total pixels\n",
    "    x = Dense(8192, activation='relu')(decoder_input)\n",
    "\n",
    "    x = Reshape((32, 32, 8))(x)\n",
    "\n",
    "    x = Conv2DTranspose(32, kernel_size=3,\n",
    "                padding='same', \n",
    "                activation='relu',\n",
    "                strides=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2DTranspose(32, kernel_size=3,\n",
    "                padding='same', \n",
    "                activation='relu',\n",
    "                strides=(2, 2))(x)\n",
    "    # reshape\n",
    "    x = Conv2D(8, 3, padding='same', activation='relu')(x)\n",
    "    x = Conv2D(1, 3,\n",
    "                padding='same', \n",
    "                activation='sigmoid')(x)\n",
    "\n",
    "    # decoder model statement\n",
    "    decoder = Model(decoder_input, x)\n",
    "    # apply the decoder to the sample from the latent distribution\n",
    "    z_decoded = decoder(z)\n",
    "    \n",
    "    # apply the custom loss to the input images and the decoded latent distribution sample\n",
    "    y = CustomVariationalLayer()([input_img, z_decoded, z_log_sigma, z_mu])\n",
    "    \n",
    "    \n",
    "    vae = Model(input_img, y)\n",
    "    vae.compile(optimizer='rmsprop', loss=None)\n",
    "    vae.summary()\n",
    "    decoder.summary()\n",
    "\n",
    "    vae.fit(x=train_data, y=None, shuffle=True, epochs=8000 + (p * 50), batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    data_path = \"potato_dataset/data_\" + str(procent) + \"/model_\" + str(model) + \"/vae_4/\" + potato_type + \"/\"\n",
    "    n = 20\n",
    "    image_size = 128\n",
    "    figure = np.zeros((image_size * n, image_size * n))\n",
    "        \n",
    "    grid = norm.ppf(np.linspace(0.00, 1.00, 25))\n",
    "    for i in range(number_of_images):\n",
    "        sample = np.array([np.random.choice(grid, size=latent_dim, replace=True)])\n",
    "        z_sample = np.tile(sample, batch_size).reshape(batch_size, latent_dim)\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        final_image = x_decoded[0].reshape(image_size, image_size)\n",
    "        plt.imsave(data_path + potato_type +\"_vae_\" + str(i) + \".jpg\", final_image)\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in range(10, 100, 10):\n",
    "    start1 = time.time()\n",
    "    for j in range(1, 11):\n",
    "        start2 = time.time()\n",
    "        hollow_data = get_train_hollow_data(p, j)\n",
    "        run_model(hollow_data, p, j, \"hollow\", int(40 + p))\n",
    "        end2 = time.time()\n",
    "        print(\"Hollow Model {} with {} procent of the data took {} second\".format(j, p, end2 - start2))\n",
    "        \n",
    "        start2 = time.time()\n",
    "        metal_data = get_train_metal_data(p, j)\n",
    "        run_model(metal_data, p, j, \"metal\", int(40 + p))\n",
    "        end2 = time.time()\n",
    "        print(\"metal Model {} with {} procent of the data took {} second\".format(j, p, end2 - start2))\n",
    "        \n",
    "        start2 = time.time()\n",
    "        perfect_data = get_train_perfect_data(p, j)\n",
    "        run_model(perfect_data, p, j, \"perfect\", int(40 + p))\n",
    "        end2 = time.time()\n",
    "        print(\"perfect Model {} with {} procent of the data took {} second\".format(j, p, end2 - start2))\n",
    "    end1 = time.time()\n",
    "    print(\"VAE for {} procent of data took {} second\".format(p, end1 - start1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
