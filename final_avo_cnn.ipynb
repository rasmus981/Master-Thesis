{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "import pickle\n",
    "import warnings\n",
    "from scipy import misc\n",
    "import scipy.ndimage\n",
    "from skimage import feature\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test_data(procent, model_number, augmented_data_list):\n",
    "    train_avo_image_folder = \"avo_dataset/data_\" + str(procent) + \"/model_\" + str(model_number) + \"/train_images/\"\n",
    "    train_avo_labels_folder = \"avo_dataset/data_\" + str(procent) + \"/model_\" + str(model_number) + \"/train_labels/\"\n",
    "    \n",
    "    test_avo_image_folder = \"avo_dataset/data_\" + str(procent) + \"/model_\" + str(model_number) + \"/test_images/\"\n",
    "    test_avo_labels_folder = \"avo_dataset/data_\" + str(procent) + \"/model_\" + str(model_number) + \"/test_labels/\"\n",
    "\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "\n",
    "    for filename in os.listdir(train_avo_image_folder):\n",
    "        if filename != \".DS_Store\" and filename != \".png\" and filename != \".ipynb_checkpoints\":\n",
    "            img = (cv2.imread(train_avo_image_folder + filename, cv2.IMREAD_GRAYSCALE) / 255.0)\n",
    "            train_X.append(img)\n",
    "            label = np.loadtxt(train_avo_labels_folder + filename[:-4] + \"_label.txt\", dtype=int)\n",
    "            train_Y.append(label)\n",
    "\n",
    "    for filename in os.listdir(test_avo_image_folder):\n",
    "        if filename != \".DS_Store\" and filename != \".png\" and filename != \".ipynb_checkpoints\":\n",
    "            img = (cv2.imread(test_avo_image_folder + filename, cv2.IMREAD_GRAYSCALE) / 255.0)\n",
    "            test_X.append(img)\n",
    "            label = np.loadtxt(test_avo_labels_folder + filename[:-4] + \"_label.txt\", dtype=int)\n",
    "            test_Y.append(label)\n",
    "\n",
    "            \n",
    "    # Augmented training data\n",
    "    for augmentation in augmented_data_list:\n",
    "        augmented_train_image_folder = \"avo_dataset/data_\" + str(procent) + \"/model_\" + str(model_number) + \"/\" + augmentation + \"_images/\"\n",
    "        augmented_train_label_folder = \"avo_dataset/data_\" + str(procent) + \"/model_\" + str(model_number) + \"/\" + augmentation + \"_labels/\"\n",
    "        \n",
    "        for filename in os.listdir(augmented_train_image_folder):\n",
    "            if filename != \".DS_Store\" and filename != \".png\" and filename != \".ipynb_checkpoints\":\n",
    "                if augmentation == \"final_vae\":\n",
    "                    img = (cv2.imread(augmented_train_image_folder + filename, cv2.IMREAD_GRAYSCALE))\n",
    "                else:\n",
    "                    img = (cv2.imread(augmented_train_image_folder + filename, cv2.IMREAD_GRAYSCALE) / 255.0)\n",
    "                train_X.append(img)\n",
    "                label = np.loadtxt(augmented_train_label_folder + filename[:-4] + \"_label.txt\", dtype=int)\n",
    "                train_Y.append(label)\n",
    "\n",
    "\n",
    "\n",
    "    # Convert lists to np array\n",
    "    np_train_images = np.array(train_X)\n",
    "\n",
    "    # Convert lists to np array\n",
    "    np_test_images = np.array(test_X)\n",
    "\n",
    "    # Data labels\n",
    "    train_labels = np.array(train_Y)\n",
    "\n",
    "    test_labels = np.array(test_Y)\n",
    "\n",
    "    train_data = np_train_images.reshape(np_train_images.shape[0], 128, 128, 1)\n",
    "    test_data = np_test_images.reshape(np_test_images.shape[0], 128, 128, 1)\n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differnce(y_true, y_pred):\n",
    "    return K.mean(np.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(train_data, train_labels, test_data, test_labels, patience):\n",
    "    \n",
    "    # Model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(8, kernel_size=3, padding='same', strides=2, activation=tf.nn.relu, input_shape=(128,128,1)))\n",
    "    model.add(Conv2D(8, kernel_size=3, padding='same', strides=2, activation=tf.nn.relu))\n",
    "    model.add(Conv2D(8, kernel_size=3, padding='same', strides=2, activation=tf.nn.relu))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation=tf.nn.relu))\n",
    "    model.add(Dense(512, activation=tf.nn.relu))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    \n",
    "    \n",
    "    # Compile Model\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss=differnce,\n",
    "                  metrics=[differnce])\n",
    "\n",
    "    \n",
    "    # Define Earlystopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "\n",
    "\n",
    "    # Get history\n",
    "    history = model.fit(x=train_data,y=train_labels, validation_data=(test_data, test_labels), shuffle=True, batch_size=32, epochs=1000, verbose=0, callbacks=[es])\n",
    "    \n",
    "    \n",
    "    # Get loss\n",
    "    history_loss = np.array(history.history['loss'])\n",
    "    history_val_loss = np.array(history.history['val_loss'])\n",
    "\n",
    "    _, train_difference = model.evaluate(train_data, train_labels, verbose=0)\n",
    "    _, test_difference = model.evaluate(test_data, test_labels, verbose=0)\n",
    "    \n",
    "    # Clear meomory\n",
    "    K.clear_session()\n",
    "    return train_difference, test_difference, np.mean(history_loss[-30:]), np.mean(history_val_loss[-30:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write name of augmentations methods\n",
    "augmentaion_list = []\n",
    "\n",
    "for p in range(10, 100, 10):\n",
    "    start = time.time()\n",
    "    final_acc = []\n",
    "    final_loss = []\n",
    "    model_loss = 0\n",
    "    model_acc = 0\n",
    "    for j in range(5):\n",
    "        model_acc = 0\n",
    "        model_loss = 0\n",
    "        train_data, train_labels, test_data, test_labels = get_train_and_test_data(p, j, augmentaion_list)\n",
    "        for i in range(5):\n",
    "            #train_data, train_labels, test_data, test_labels = get_train_and_test_data(p, j + 1)\n",
    "            train_difference, test_difference, loss, val_loss  = run_model(train_data, train_labels, test_data, test_labels, 40 + p)\n",
    "            print(\"Run {} with model {} got train acc {} on test acc {}\".format(i, j + 1, train_difference, test_difference))\n",
    "            print(\"Run {} with model {} got train loss {} and val_loss {}\".format(i, j + 1, loss, val_loss))\n",
    "            \n",
    "            model_acc += test_difference\n",
    "            model_loss += val_loss\n",
    "            \n",
    "        final_acc.append(model_acc)\n",
    "        final_loss.append(model_loss)\n",
    "        \n",
    "    for i in range(5):\n",
    "        print(\"Model {} got an difference on: {}\".format(i, final_acc[i] / 5.0))\n",
    "        print(\"Model {} got an val loss on: {}\".format(i, final_acc[i] / 5.0))\n",
    "        \n",
    "    print(\"Using {} procent of data to training our model got an acc {}\".format(p, sum(final_acc) / 25))\n",
    "    print(\"Using {} procent of data to training our model got an loss {}\".format(p, sum(final_loss) / 25))\n",
    "    end = time.time()\n",
    "    print(end - start)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
